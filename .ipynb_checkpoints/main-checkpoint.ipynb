{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac3ae09-ae05-4a8f-93d1-4d5080e35efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d544c9-d7d3-4516-b59f-978a1b4595bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env, spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import load_model\n",
    "from tabulate import tabulate\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5095c32c-e2b0-4b73-a8c7-ea3e2fac4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea496941-4692-4ffd-bf7e-b7957dd40642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34.3,  0. ,  0. ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ring_buffer = np.array([0, 0, 0], dtype='float32')\n",
    "ring_buffer[0] = 34.3\n",
    "ring_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897cc19c-3426-4ddd-8501-84ac12ec6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pouringEnv(Env):\n",
    "  def __init__(self, path=None):\n",
    "    self.action_space = spaces.Discrete(21) \n",
    "    self.state_shape = (2,)\n",
    "    self.action_shape = self.action_space.n\n",
    "    self.gamma=0.99 \n",
    "    self.alpha=1e-4 \n",
    "    self.learning_rate=0.01 \n",
    "\n",
    "    # Container parameters\n",
    "    self.h_c = 10.0\n",
    "    self.r_c = 2.5\n",
    "    self.h_l = self.h_c/2.0\n",
    "    self.v_l = self.h_l*math.pi*self.r_c**2\n",
    "    self.w_E = (2/3)*self.v_l\n",
    "    \n",
    "    self.alpha_lamda = 1.5708\n",
    "    self.alpha_gamma = 0.6\n",
    "    self.alpha_beta = 2.5\n",
    "    self.ring_buffer = np.array([0, 0, 0], dtype='float32')\n",
    "    \n",
    "    self.alpha_spill = math.atan((self.h_c-self.h_l)/self.r_c)\n",
    "    self.time_spill = -math.sqrt(-np.log(self.alpha_spill/self.alpha_lamda)/self.alpha_gamma) + self.alpha_beta\n",
    "    self.v_goal = 0\n",
    "    self.vt = 0\n",
    "    self.time = np.arange(0,self.alpha_beta,0.0333)\n",
    "    self.current_alpha = 0\n",
    "    self.Dalpha_i = 0 \n",
    "    \n",
    "    if not path:\n",
    "      self.model=self._create_model() \n",
    "    else:\n",
    "      self.model=self.load_model(path)\n",
    "\n",
    "  def _create_model(self):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(2, input_shape=self.state_shape, activation=\"relu\"))\n",
    "    model.add(Dense(12, activation=\"relu\"))\n",
    "    model.add(Dense(self.action_shape, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "    return model\n",
    "\n",
    "  def get_action(self, state):\n",
    "    state=state.reshape([1, state.shape[0]])\n",
    "    action_probability_distribution=self.model.predict(state, verbose=0).flatten()\n",
    "    action=np.random.choice(self.action_shape,1,p=action_probability_distribution)[0]\n",
    "    return action, action_probability_distribution\n",
    "\n",
    "  def get_alpha(self, time):\n",
    "    return self.alpha_lamda*np.exp(-self.alpha_gamma*(time-self.alpha_beta)**2)\n",
    "\n",
    "  def V_stack(self, itime):\n",
    "    t = np.arange(0, itime, 0.0333)\n",
    "    alphas = self.get_alpha(t)\n",
    "    h1 = self.h_l + self.r_c*np.tan(alphas)\n",
    "    h2 = self.h_c - h1\n",
    "    \n",
    "    if (h2[-1] >= 0.001):\n",
    "        v = np.zeros((len(t)))\n",
    "    else:\n",
    "        a = self.time_spill/10.0\n",
    "        h = (self.r_c)*np.exp(-((t-self.time_spill)**2)/(2*a**2))\n",
    "        v = np.tan(alphas)*h**2\n",
    "\n",
    "    return np.sum(v)\n",
    "\n",
    "  def reset(self):\n",
    "    time_i = np.random.choice(self.time)\n",
    "    alpha_i = self.get_alpha(time_i)    \n",
    "    self.current_alpha = alpha_i\n",
    "    h1 = self.h_l + self.r_c*math.tan(alpha_i)\n",
    "    h2 = self.h_c - h1\n",
    "    k = np.random.choice(np.arange(0.5, 1.55, 0.05))\n",
    "    self.v_goal = self.w_E*k\n",
    "    \n",
    "    if (h2 >= 0.001):\n",
    "        h_i = -(self.h_c*math.sin(alpha_i+math.pi/2) - h1*math.sin(alpha_i+math.pi/2))\n",
    "        v_i = 0\n",
    "        v_0_i = 0\n",
    "        v_i_n = 0\n",
    "        d_i = (self.v_goal - v_0_i) - v_i\n",
    "    else:\n",
    "        a = self.time_spill/10.0\n",
    "        h_i = (self.r_c)*np.exp(-((time_i-self.time_spill)**2)/(2*a**2))\n",
    "        v_i = math.tan(alpha_i)*h_i**2\n",
    "        v_i_n = self.w_E*(v_i/self.V_stack(self.alpha_beta))\n",
    "        v_0_i = self.V_stack(time_i)\n",
    "        d_i = (self.v_goal - v_0_i) - v_i\n",
    "\n",
    "    print(\"---------Episode information---------\")\n",
    "    data = [[time_i, alpha_i, v_i, h_i]]\n",
    "    print(tabulate(data, headers=[\"time_i\", \"alpha_i\", \"V_i\", \"h_i\"]))\n",
    "    print(\"-------------------------------------\")\n",
    "    data2 = [[self.time_spill, self.alpha_spill, self.v_goal]]\n",
    "    print(tabulate(data2, headers=[\"time_s\", \"alpha_s\", \"Vg\"]))\n",
    "    print(\"----------------------------\")\n",
    "    data3 = [[self.v_l, self.h_c, self.r_c, self.h_l]]\n",
    "    print(tabulate(data3, headers=[\"Vl\", \"hc\", \"rc\", \"hl\"]))\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    new_state = np.array([v_i_n, d_i])\n",
    "    return new_state\n",
    "\n",
    "  def next_step(self, action):\n",
    "    action = action - 10\n",
    "    if (action < 0):\n",
    "      alpha_i = (-0.5149425*action**2)*(math.pi/180.0)\n",
    "    else:\n",
    "      alpha_i =  (0.05149425*action**2)*(math.pi/180.0)\n",
    "    self.Dalpha_i = alpha_i\n",
    "    self.current_alpha = alpha_i + self.current_alpha\n",
    "    time_i = -math.sqrt(-np.log(self.current_alpha/self.alpha_lamda)/self.alpha_gamma) + self.alpha_beta\n",
    "    h1 = self.h_l + self.r_c*math.tan(self.current_alpha)\n",
    "    h2 = self.h_c - h1\n",
    "    \n",
    "    if (h2 >= 0.001):\n",
    "        h_i = -(self.h_c*math.sin(self.current_alpha+math.pi/2) - h1*math.sin(self.current_alpha+math.pi/2))\n",
    "        v_i = 0\n",
    "        v_0_i = 0\n",
    "        v_i_n = 0\n",
    "        d_i = (self.v_goal - v_0_i) - v_i\n",
    "    else:\n",
    "        a = self.time_spill/10.0\n",
    "        h_i = (self.r_c)*np.exp(-((time_i-self.time_spill)**2)/(2*a**2))\n",
    "        v_i = math.tan(self.current_alpha)*h_i**2\n",
    "        v_i_n = self.w_E*(v_i/self.V_stack(self.alpha_beta))\n",
    "        v_0_i = self.V_stack(time_i)\n",
    "        d_i = (self.v_goal - v_0_i) - v_i\n",
    "    \n",
    "    #print(\"v_i+1: %3.2f, d_i+1: %3.2f, a_i+1: %3.2f\" % (v_i_n, d_i, self.current_alpha))\n",
    "    new_state = np.array([v_i_n, d_i])\n",
    "    done = False\n",
    "    deltaV = self.v_goal - (v_0_i+v_i)\n",
    "\n",
    "    if (self.ring_buffer[0]*self.ring_buffer[1] >= 0):\n",
    "        Delta = 1\n",
    "    else:\n",
    "        Delta = -1\n",
    "\n",
    "    if (d_i > 0):\n",
    "        Reward = Delta + v_i**2 - math.log(d_i/self.v_goal)\n",
    "    else:\n",
    "        Reward = Delta + v_i**2 - 100.0\n",
    "        \n",
    "    # Shift buffer\n",
    "    # Di,   Di-1,  Di-1\n",
    "    self.ring_buffer[2] = self.ring_buffer[1] \n",
    "    # Di,   Di,    Di-1\n",
    "    self.ring_buffer[1] = self.ring_buffer[0]\n",
    "    # Di+1, Di,    Di-1 \n",
    "    self.ring_buffer[0] = self.Dalpha_i\n",
    "            \n",
    "    if ((deltaV) <= 0.5):\n",
    "      done = True\n",
    "      print(\"Done!\")\n",
    "    return new_state, done, Reward\n",
    "\n",
    "  def train(self, episodes):\n",
    "    for episode in range(episodes):\n",
    "        state = self.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, prob = self.get_action(state)\n",
    "            next_state, done, reward = self.next_step(action)\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8b5a3f-a45e-4ebb-a743-7dfb071bd2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.044522437723423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59f0dec-6641-495e-9996-257f539c18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = pouringEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb071cd-2e17-4453-b135-68b3512f71fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstate = agent.reset()\\naction, prob = agent.get_action(state)\\ndone=False\\nwhile not done:\\n  action, prob = agent.get_action(state)\\n  next_state, done, deltaV = agent.next_step(action)\\n  state = next_state\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "state = agent.reset()\n",
    "action, prob = agent.get_action(state)\n",
    "done=False\n",
    "while not done:\n",
    "  action, prob = agent.get_action(state)\n",
    "  next_state, done, deltaV = agent.next_step(action)\n",
    "  state = next_state\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d27776-37b0-4bfd-abe3-eec4296a4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Episode information---------\n",
      "  time_i    alpha_i    V_i        h_i\n",
      "--------  ---------  -----  ---------\n",
      "   1.665    1.03381      0  -0.409614\n",
      "-------------------------------------\n",
      "  time_s    alpha_s       Vg\n",
      "--------  ---------  -------\n",
      " 1.73646    1.10715  71.9948\n",
      "----------------------------\n",
      "     Vl    hc    rc    hl\n",
      "-------  ----  ----  ----\n",
      "98.1748    10   2.5     5\n",
      "-------------------------\n",
      "Done!\n",
      "---------Episode information---------\n",
      "  time_i    alpha_i    V_i        h_i\n",
      "--------  ---------  -----  ---------\n",
      "  1.6983    1.06817      0  -0.217816\n",
      "-------------------------------------\n",
      "  time_s    alpha_s       Vg\n",
      "--------  ---------  -------\n",
      " 1.73646    1.10715  81.8123\n",
      "----------------------------\n",
      "     Vl    hc    rc    hl\n",
      "-------  ----  ----  ----\n",
      "98.1748    10   2.5     5\n",
      "-------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "agent.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5e397-1083-49ef-9cbf-165aa46eee63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
